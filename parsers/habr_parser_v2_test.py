#!/usr/bin/env python3
"""
Ð¢ÐµÑÑ‚Ð¾Ð²Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ Habr Ð¿Ð°Ñ€ÑÐµÑ€Ð° Ñ Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð¾Ð¼ 2 (Ð±ÐµÐ· Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¾Ð²)
"""

import os
import sys
import time
import json
import logging
import requests
from datetime import datetime
from urllib.parse import urljoin, quote
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Any

try:
    from text_formatter_v2 import extract_content_without_headers, clean_text
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(__file__))
    from text_formatter_v2 import extract_content_without_headers, clean_text

class HabrParserV2Test:
    """
    Ð¢ÐµÑÑ‚Ð¾Ð²Ð°Ñ Ð²ÐµÑ€ÑÐ¸Ñ Habr Ð¿Ð°Ñ€ÑÐµÑ€Ð° Ñ Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð¾Ð¼ 2 Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð°
    """
    
    def __init__(self):
        self.base_url = 'https://career.habr.com'
        self.search_url = 'https://career.habr.com/vacancies'
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def search_vacancies(self, query: str, pages: int = 1) -> List[Dict[str, Any]]:
        """
        ÐŸÐ¾Ð¸ÑÐº Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¹ Ð½Ð° Habr Career
        """
        vacancies = []
        
        for page in range(1, pages + 1):
            try:
                self.logger.info(f"ðŸ” ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ {page} Ð´Ð»Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° '{query}'")
                
                # ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¿Ð¾Ð¸ÑÐºÐ°
                params = {
                    'q': query,
                    'type': 'all',
                    'page': page
                }
                
                response = self.session.get(self.search_url, params=params, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ÐŸÐ¾Ð¸ÑÐº ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐµÐº Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¹
                vacancy_cards = soup.select('.vacancy-card, .vacancy-list-item')
                
                if not vacancy_cards:
                    self.logger.warning(f"âš ï¸ ÐÐ° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ðµ {page} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¹")
                    break
                
                self.logger.info(f"ðŸ“‹ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(vacancy_cards)} Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¹ Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ðµ {page}")
                
                for card in vacancy_cards:
                    try:
                        vacancy = self.parse_vacancy_card(card)
                        if vacancy:
                            vacancies.append(vacancy)
                    except Exception as e:
                        self.logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸: {e}")
                        continue
                
                # ÐŸÐ°ÑƒÐ·Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸
                time.sleep(1)
                
            except Exception as e:
                self.logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ {page}: {e}")
                continue
        
        self.logger.info(f"âœ… Ð’ÑÐµÐ³Ð¾ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ {len(vacancies)} Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¹")
        return vacancies
    
    def parse_vacancy_card(self, card) -> Optional[Dict[str, Any]]:
        """
        ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸
        """
        try:
            # Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº Ð¸ ÑÑÑ‹Ð»ÐºÐ°
            title_link = card.select_one('.vacancy-card__title a, .vacancy-card-title a, h3 a')
            if not title_link:
                return None
            
            title = title_link.get_text(strip=True)
            url = urljoin(self.base_url, title_link.get('href', ''))
            
            # ÐšÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñ
            company_elem = card.select_one('.vacancy-card__company-title a, .vacancy-card__company a, .company-name a')
            company = company_elem.get_text(strip=True) if company_elem else 'ÐÐµ ÑƒÐºÐ°Ð·Ð°Ð½Ð¾'
            
            # Ð—Ð°Ñ€Ð¿Ð»Ð°Ñ‚Ð°
            salary_elem = card.select_one('.vacancy-card__salary, .salary')
            salary = salary_elem.get_text(strip=True) if salary_elem else ''
            
            # Ð›Ð¾ÐºÐ°Ñ†Ð¸Ñ
            location_elem = card.select_one('.vacancy-card__meta, .location')
            location = location_elem.get_text(strip=True) if location_elem else ''
            
            # Ð”Ð°Ñ‚Ð° Ð¿ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
            date_elem = card.select_one('.vacancy-card__date, .date')
            published_at = date_elem.get_text(strip=True) if date_elem else ''
            
            return {
                'title': title,
                'company': company,
                'url': url,
                'salary': salary,
                'location': location,
                'published_at': published_at,
                'source': 'habr'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸: {e}")
            return None
    
    def get_vacancy_details(self, url: str) -> Dict[str, Any]:
        """
        ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð¾ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸ Ñ Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð¾Ð¼ 2
        """
        try:
            self.logger.info(f"ðŸ” ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð´ÐµÑ‚Ð°Ð»ÐµÐ¹ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸: {url}")
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Ð¡ÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ñ
            description_selectors = [
                '.vacancy-description',
                '.basic-section--appearance-vacancy-description',
                '.vacancy-section',
                '.job-description'
            ]
            
            description = ""
            for selector in description_selectors:
                element = soup.select_one(selector)
                if element:
                    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚ 2: Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð°Ð±Ð·Ð°Ñ†Ñ‹ Ð¸ ÑÐ¿Ð¸ÑÐºÐ¸ Ð±ÐµÐ· Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¾Ð²
                    description = extract_content_without_headers(element)
                    if description:
                        break
            
            # ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð°
            description = clean_text(description)
            
            return {
                'full_description': description,
                'requirements': '',  # ÐÐµ Ñ€Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° Ð±Ð»Ð¾ÐºÐ¸ Ð² Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ðµ 2
                'tasks': '',
                'benefits': '',
                'conditions': ''
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð´ÐµÑ‚Ð°Ð»ÐµÐ¹ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸ {url}: {e}")
            return {
                'full_description': '',
                'requirements': '',
                'tasks': '',
                'benefits': '',
                'conditions': ''
            }
    
    def parse_vacancies(self, query: str, pages: int = 1) -> List[Dict[str, Any]]:
        """
        ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¹
        """
        self.logger.info(f"ðŸš€ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Habr Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð¼ '{query}', ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†: {pages}")
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¹
        vacancies = self.search_vacancies(query, pages)
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´ÐµÑ‚Ð°Ð»Ð¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸
        detailed_vacancies = []
        for i, vacancy in enumerate(vacancies, 1):
            self.logger.info(f"ðŸ“„ ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð²Ð°ÐºÐ°Ð½ÑÐ¸ÑŽ {i}/{len(vacancies)}: {vacancy['title']}")
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´ÐµÑ‚Ð°Ð»Ð¸
            details = self.get_vacancy_details(vacancy['url'])
            
            # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
            full_vacancy = {
                **vacancy,
                **details,
                'id': f"habr_{hash(vacancy['url'])}",
                'created_at': datetime.now().isoformat()
            }
            
            detailed_vacancies.append(full_vacancy)
            
            # ÐŸÐ°ÑƒÐ·Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸
            time.sleep(1)
        
        self.logger.info(f"âœ… ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½. ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ {len(detailed_vacancies)} Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¹")
        return detailed_vacancies


def main():
    """
    Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð°Ñ€ÑÐµÑ€Ð°
    """
    parser = HabrParserV2Test()
    
    print("ðŸ§ª Ð¢Ð•Ð¡Ð¢Ð˜Ð ÐžÐ’ÐÐÐ˜Ð• HABR ÐŸÐÐ Ð¡Ð•Ð Ð Ð¡ Ð’ÐÐ Ð˜ÐÐÐ¢ÐžÐœ 2")
    print("=" * 50)
    
    # Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³
    vacancies = parser.parse_vacancies('Ð´Ð¸Ð·Ð°Ð¹Ð½ÐµÑ€', pages=1)
    
    print(f"\nðŸ“Š Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹:")
    print(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¹: {len(vacancies)}")
    
    if vacancies:
        print(f"\nðŸ“‹ ÐŸÐµÑ€Ð²Ð°Ñ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ñ:")
        first = vacancies[0]
        print(f"Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº: {first['title']}")
        print(f"ÐšÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñ: {first['company']}")
        print(f"URL: {first['url']}")
        print(f"ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ (Ð¿ÐµÑ€Ð²Ñ‹Ðµ 200 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²):")
        print(first['full_description'][:200] + "..." if len(first['full_description']) > 200 else first['full_description'])


if __name__ == "__main__":
    main()










