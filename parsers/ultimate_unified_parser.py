# parsers/ultimate_unified_parser.py

import asyncio
import logging
import sqlite3
import argparse
import json
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    from enhanced_hh_parser import EnhancedHHParser
    from enhanced_habr_parser import EnhancedHabrParser
    from vacancy_filter import VacancyFilter
    from caching_system import CachingSystem, CachedParser
    from monitoring_system import MonitoringSystem, MonitoredParser
    
    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç Playwright
    try:
        from playwright_hh_parser import PlaywrightHHParser
        PLAYWRIGHT_AVAILABLE = True
    except ImportError:
        PLAYWRIGHT_AVAILABLE = False
        
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(__file__))
    from enhanced_hh_parser import EnhancedHHParser
    from enhanced_habr_parser import EnhancedHabrParser
    from vacancy_filter import VacancyFilter
    from caching_system import CachingSystem, CachedParser
    from monitoring_system import MonitoringSystem, MonitoredParser
    
    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç Playwright
    try:
        from playwright_hh_parser import PlaywrightHHParser
        PLAYWRIGHT_AVAILABLE = True
    except ImportError:
        PLAYWRIGHT_AVAILABLE = False

class UltimateUnifiedParser(CachedParser, MonitoredParser):
    """
    –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å:
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∏ –∞–ª–µ—Ä—Ç–∞–º–∏
    - Fallback –º–µ–∂–¥—É –ø–∞—Ä—Å–µ—Ä–∞–º–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º –ª—É—á—à–µ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
    - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
    - –ü–æ–¥—Ä–æ–±–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π
    """
    
    def __init__(self, db_path: str = "data/vacancies.db", use_playwright: bool = False):
        CachedParser.__init__(self, cache_ttl=1800)  # 30 –º–∏–Ω—É—Ç –∫—ç—à
        MonitoredParser.__init__(self)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (—Å–Ω–∞—á–∞–ª–∞!)
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è Windows
        import sys
        if sys.platform == 'win32':
            import os
            os.environ['PYTHONIOENCODING'] = 'utf-8'
        
        self.db_path = db_path
        self.use_playwright = use_playwright
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–æ–≤
        self.parsers = {}
        self._init_parsers()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º
        self.filter = VacancyFilter()
        self.cache = CachingSystem()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_found': 0,
            'total_saved': 0,
            'total_filtered': 0,
            'total_cached': 0,
            'by_source': {},
            'start_time': datetime.now(),
            'performance': {}
        }
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ fallback
        self.fallback_enabled = True
        self.parser_priorities = ['habr', 'hh_enhanced', 'hh_playwright']
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
        self.init_database()
    
    def _init_parsers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤"""
        try:
            # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã
            self.parsers['habr'] = EnhancedHabrParser()
            self.parsers['hh_enhanced'] = EnhancedHHParser()
            
            # Playwright –ø–∞—Ä—Å–µ—Ä (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –Ω—É–∂–µ–Ω)
            if self.use_playwright and PLAYWRIGHT_AVAILABLE:
                self.parsers['hh_playwright'] = PlaywrightHHParser()
            elif self.use_playwright and not PLAYWRIGHT_AVAILABLE:
                self.logger.warning("Playwright requested but not available. Install with: pip install playwright")
            
            self.logger.info(f"Initialized {len(self.parsers)} parsers: {list(self.parsers.keys())}")
            
        except Exception as e:
            self.logger.error(f"Error initializing parsers: {str(e)}")
            raise
    
    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å—Ö–µ–º–æ–π"""
        try:
            db_dir = Path(self.db_path).parent
            db_dir.mkdir(parents=True, exist_ok=True)
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –≤–∞–∫–∞–Ω—Å–∏–π (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS vacancies (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    external_id TEXT,
                    source TEXT NOT NULL,
                    url TEXT UNIQUE NOT NULL,
                    title TEXT NOT NULL,
                    company TEXT,
                    location TEXT,
                    description TEXT,
                    salary_min INTEGER,
                    salary_max INTEGER,
                    salary_currency TEXT,
                    published_at DATETIME,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    ai_specialization TEXT,
                    ai_employment TEXT,
                    ai_experience TEXT,
                    ai_technologies TEXT,
                    ai_salary_min INTEGER,
                    ai_salary_max INTEGER,
                    ai_remote TEXT,
                    ai_relevance_score REAL,
                    ai_summary TEXT,
                    is_approved BOOLEAN DEFAULT 0,
                    is_rejected BOOLEAN DEFAULT 0,
                    moderation_notes TEXT,
                    moderated_at DATETIME,
                    moderated_by TEXT,
                    full_description TEXT,
                    requirements TEXT,
                    tasks TEXT,
                    benefits TEXT,
                    conditions TEXT,
                    company_logo TEXT,
                    company_url TEXT,
                    employment_type TEXT,
                    experience_level TEXT,
                    remote_type TEXT,
                    
                    -- –ù–æ–≤—ã–µ –ø–æ–ª—è –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
                    parser_used TEXT,
                    parse_time REAL,
                    quality_score REAL,
                    cache_hit BOOLEAN DEFAULT FALSE,
                    retry_count INTEGER DEFAULT 0
                )
            """)
            
            # –¢–∞–±–ª–∏—Ü–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–∞—Ä—Å–µ—Ä–æ–≤
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS parser_performance (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    source TEXT NOT NULL,
                    query TEXT,
                    page INTEGER,
                    response_time REAL,
                    items_found INTEGER,
                    success_rate REAL,
                    cache_hit_rate REAL,
                    error_count INTEGER DEFAULT 0
                )
            """)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            try:
                cursor.execute("ALTER TABLE vacancies ADD COLUMN parser_used TEXT")
            except sqlite3.OperationalError:
                pass  # –ö–æ–ª–æ–Ω–∫–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            
            try:
                cursor.execute("ALTER TABLE vacancies ADD COLUMN parse_time REAL")
            except sqlite3.OperationalError:
                pass
                
            try:
                cursor.execute("ALTER TABLE vacancies ADD COLUMN quality_score REAL")
            except sqlite3.OperationalError:
                pass
                
            try:
                cursor.execute("ALTER TABLE vacancies ADD COLUMN cache_hit BOOLEAN DEFAULT FALSE")
            except sqlite3.OperationalError:
                pass
                
            try:
                cursor.execute("ALTER TABLE vacancies ADD COLUMN retry_count INTEGER DEFAULT 0")
            except sqlite3.OperationalError:
                pass
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_source ON vacancies(source)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_created_at ON vacancies(created_at)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_quality_score ON vacancies(quality_score)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_parser_performance ON parser_performance(timestamp, source)")
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"Database initialized: {self.db_path}")
            
        except Exception as e:
            self.logger.error(f"Database initialization failed: {str(e)}")
            raise
    
    async def parse_source_with_fallback(self, source: str, query: str, pages: int, extract_details: bool = True) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å fallback –Ω–∞ –¥—Ä—É–≥–∏–µ –ø–∞—Ä—Å–µ—Ä—ã"""
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–∞—Ä—Å–µ—Ä–æ–≤ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        if source == 'hh':
            parser_options = ['hh_enhanced']
            if self.use_playwright and PLAYWRIGHT_AVAILABLE:
                parser_options.append('hh_playwright')
        elif source == 'habr':
            parser_options = ['habr']
        else:
            parser_options = [source] if source in self.parsers else []
        
        for parser_name in parser_options:
            if parser_name not in self.parsers:
                continue
                
            parser = self.parsers[parser_name]
            start_time = time.time()
            
            try:
                self.logger.info(f"Trying parser {parser_name} for {source}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
                cache_key_params = {'extract_details': extract_details}
                cached_data = self.cache.get(source, query, 1, **cache_key_params) if pages == 1 else None
                
                if cached_data:
                    self.stats['total_cached'] += len(cached_data)
                    self.record_success(source, query, 1, 0, len(cached_data))
                    return cached_data
                
                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–∞—Ä—Å–∏–Ω–≥
                if asyncio.iscoroutinefunction(parser.parse_vacancies):
                    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä (Playwright)
                    vacancies = await parser.parse_vacancies(query, pages, extract_details)
                else:
                    # –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä
                    vacancies = parser.parse_vacancies(query, pages, extract_details)
                
                parse_time = time.time() - start_time
                
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —É—Å–ø–µ—Ö
                self.record_success(source, query, pages, parse_time, len(vacancies))
                
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                for vacancy in vacancies:
                    vacancy['parser_used'] = parser_name
                    vacancy['parse_time'] = parse_time / len(vacancies) if vacancies else 0
                    vacancy['quality_score'] = self._calculate_quality_score(vacancy)
                
                # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if vacancies and pages == 1:
                    self.cache.set(source, query, 1, vacancies, self.cache_ttl, **cache_key_params)
                
                self.logger.info(f"Successfully parsed {len(vacancies)} vacancies with {parser_name}")
                return vacancies
                
            except Exception as e:
                parse_time = time.time() - start_time
                error_msg = str(e)
                
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É
                self.record_failure(source, error_msg, query, pages)
                
                self.logger.warning(f"Parser {parser_name} failed for {source}: {error_msg}")
                
                # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞—Ä—Å–µ—Ä, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π
                if parser_name != parser_options[-1]:
                    self.logger.info(f"Falling back to next parser for {source}")
                    continue
                else:
                    self.logger.error(f"All parsers failed for {source}")
                    return []
        
        return []
    
    def _calculate_quality_score(self, vacancy: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∞–∫–∞–Ω—Å–∏–∏"""
        score = 0.0
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è (40% –æ—Ç –æ—Ü–µ–Ω–∫–∏)
        if vacancy.get('title'):
            score += 10
        if vacancy.get('company'):
            score += 10
        if vacancy.get('location'):
            score += 10
        if vacancy.get('url'):
            score += 10
        
        # –û–ø–∏—Å–∞–Ω–∏–µ (30% –æ—Ç –æ—Ü–µ–Ω–∫–∏)
        full_desc = vacancy.get('full_description', '')
        if full_desc:
            if len(full_desc) > 500:
                score += 30
            elif len(full_desc) > 200:
                score += 20
            else:
                score += 10
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (20% –æ—Ç –æ—Ü–µ–Ω–∫–∏)
        for field in ['requirements', 'tasks', 'benefits', 'conditions']:
            if vacancy.get(field):
                score += 5
        
        # –ó–∞—Ä–ø–ª–∞—Ç–∞ (10% –æ—Ç –æ—Ü–µ–Ω–∫–∏)
        if vacancy.get('salary_min') or vacancy.get('salary_max'):
            score += 10
        
        return min(score, 100.0)
    
    async def parse_all_sources(self, 
                               query: str = '–¥–∏–∑–∞–π–Ω–µ—Ä',
                               pages_per_source: int = 3,
                               extract_details: bool = True,
                               sources: Optional[List[str]] = None) -> Dict[str, List[Dict[str, Any]]]:
        """–£–º–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        
        if sources is None:
            sources = ['habr', 'hh']
        
        self.logger.info(f"Starting ultimate parsing: {sources}, query='{query}', pages={pages_per_source}")
        
        results = {}
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        if len(sources) > 1:
            tasks = []
            for source in sources:
                task = self.parse_source_with_fallback(source, query, pages_per_source, extract_details)
                tasks.append((source, task))
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–¥–∞—á–∏
            for source, task in tasks:
                try:
                    if asyncio.iscoroutine(task):
                        vacancies = await task
                    else:
                        vacancies = task
                    results[source] = vacancies
                except Exception as e:
                    self.logger.error(f"Failed to parse {source}: {str(e)}")
                    results[source] = []
        else:
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            for source in sources:
                results[source] = await self.parse_source_with_fallback(source, query, pages_per_source, extract_details)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        for source, vacancies in results.items():
            self.stats['by_source'][source] = {
                'found': len(vacancies),
                'saved': 0,
                'filtered': 0,
                'avg_quality': sum(v.get('quality_score', 0) for v in vacancies) / max(len(vacancies), 1)
            }
            self.stats['total_found'] += len(vacancies)
        
        return results
    
    def save_vacancy_enhanced(self, vacancy: Dict[str, Any]) -> bool:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            cursor.execute("SELECT id FROM vacancies WHERE url = ?", (vacancy['url'],))
            if cursor.fetchone():
                self.logger.debug(f"Vacancy already exists: {vacancy.get('external_id', 'unknown')}")
                conn.close()
                return False
            
            # –í–†–ï–ú–ï–ù–ù–û –û–¢–ö–õ–Æ–ß–ê–ï–ú –§–ò–õ–¨–¢–†–ê–¶–ò–Æ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏
            # is_relevant, filter_reason = self.filter.is_vacancy_relevant(vacancy)
            # 
            # if not is_relevant:
            #     self.logger.info(f"üö´ Filtered: {vacancy.get('title', 'Unknown')} - {filter_reason}")
            #     self.stats['total_filtered'] += 1
            #     if vacancy['source'] in self.stats['by_source']:
            #         self.stats['by_source'][vacancy['source']]['filtered'] += 1
            #     conn.close()
            #     return False
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            insert_data = {
                'external_id': vacancy.get('external_id', ''),
                'source': vacancy.get('source', ''),
                'url': vacancy.get('url', ''),
                'title': vacancy.get('title', ''),
                'company': vacancy.get('company', ''),
                'location': vacancy.get('location', ''),
                'description': vacancy.get('description', ''),
                'salary_min': vacancy.get('salary_min'),
                'salary_max': vacancy.get('salary_max'),
                'salary_currency': vacancy.get('salary_currency'),
                'published_at': vacancy.get('published_at'),
                'full_description': vacancy.get('full_description', ''),
                'requirements': vacancy.get('requirements', ''),
                'tasks': vacancy.get('tasks', ''),
                'benefits': vacancy.get('benefits', ''),
                'conditions': vacancy.get('conditions', ''),
                'employment_type': vacancy.get('employment_type'),
                'experience_level': vacancy.get('experience_level'),
                'remote_type': vacancy.get('remote_type'),
                'company_logo': vacancy.get('company_logo'),
                'company_url': vacancy.get('company_url'),
                
                # AI –ø–æ–ª—è
                'ai_specialization': 'design',
                'ai_employment': json.dumps([vacancy.get('employment_type', 'full-time')]),
                'ai_experience': vacancy.get('experience_level', 'any'),
                'ai_remote': vacancy.get('remote_type') == 'remote',
                'ai_relevance_score': 0.8,
                
                # –ù–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                'parser_used': vacancy.get('parser_used', 'unknown'),
                'parse_time': vacancy.get('parse_time', 0),
                'quality_score': vacancy.get('quality_score', 0),
                'cache_hit': vacancy.get('cache_hit', False)
            }
            
            # SQL –≤—Å—Ç–∞–≤–∫–∞
            columns = ', '.join(insert_data.keys())
            placeholders = ', '.join(['?' for _ in insert_data])
            
            cursor.execute(
                f"INSERT INTO vacancies ({columns}) VALUES ({placeholders})",
                list(insert_data.values())
            )
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"‚úÖ Saved: {vacancy.get('title', 'Unknown')} - {vacancy.get('company', 'Unknown')}")
            self.stats['total_saved'] += 1
            if vacancy['source'] in self.stats['by_source']:
                self.stats['by_source'][vacancy['source']]['saved'] += 1
            
            return True
            
        except Exception as e:
            self.logger.error(f"Error saving vacancy: {str(e)}")
            return False
    
    async def run_full_parsing(self, 
                              query: str = '–¥–∏–∑–∞–π–Ω–µ—Ä',
                              pages_per_source: int = 3,
                              extract_details: bool = True,
                              sources: Optional[List[str]] = None) -> Dict[str, Any]:
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π"""
        
        start_time = time.time()
        
        try:
            # –ü–∞—Ä—Å–∏–Ω–≥
            results = await self.parse_all_sources(query, pages_per_source, extract_details, sources)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            for source, vacancies in results.items():
                saved_count = 0
                for vacancy in vacancies:
                    if self.save_vacancy_enhanced(vacancy):
                        saved_count += 1
                
                self.logger.info(f"{source}: saved {saved_count}/{len(vacancies)} vacancies")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_time = time.time() - start_time
            
            final_stats = {
                'runtime_seconds': round(total_time, 2),
                'total_found': self.stats['total_found'],
                'total_saved': self.stats['total_saved'],
                'total_filtered': self.stats['total_filtered'],
                'total_cached': self.stats['total_cached'],
                'success_rate': round((self.stats['total_saved'] / max(self.stats['total_found'], 1)) * 100, 2),
                'by_source': self.stats['by_source'],
                'cache_stats': self.cache.get_cache_stats(),
                'health_status': self.monitor.get_health_status()
            }
            
            return final_stats
            
        except Exception as e:
            self.logger.error(f"Critical error in full parsing: {str(e)}")
            return {'error': str(e)}
    
    def print_comprehensive_report(self, stats: Dict[str, Any]):
        """–ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        
        try:
            print("\n" + "="*80)
            print("COMPREHENSIVE PARSING REPORT")
            print("="*80)
            
            if "error" in stats:
                print(f"Error: {stats['error']}")
                return
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            print(f"Runtime: {stats['runtime_seconds']}s")
            print(f"Found: {stats['total_found']} | Saved: {stats['total_saved']} | Filtered: {stats['total_filtered']}")
            print(f"Cached: {stats['total_cached']} | Success Rate: {stats['success_rate']}%")
            
            # –ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            print(f"\nBY SOURCE:")
            for source, source_stats in stats['by_source'].items():
                quality_indicator = "OK" if source_stats['avg_quality'] > 80 else "WARN" if source_stats['avg_quality'] > 60 else "FAIL"
                print(f"  {source:12}: {source_stats['found']:3} found -> {source_stats['saved']:3} saved "
                      f"({source_stats['filtered']:2} filtered) {quality_indicator} {source_stats['avg_quality']:.1f}%")
            
            # –ö—ç—à —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            cache_stats = stats.get('cache_stats', {})
            if cache_stats and 'hit_ratio' in cache_stats:
                print(f"\nCACHE PERFORMANCE:")
                print(f"  Hit Ratio: {cache_stats['hit_ratio']}% | Active Entries: {cache_stats['active_entries']}")
                print(f"  Cache Size: {cache_stats['cache_size_mb']} MB")
            
            # –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
            health = stats.get('health_status', {})
            if health and 'overall_status' in health:
                status_indicator = {"healthy": "OK", "warning": "WARN", "degraded": "DEGRADED", "critical": "CRITICAL"}.get(health['overall_status'], "UNKNOWN")
                print(f"\nSYSTEM HEALTH: {status_indicator} {health['overall_status'].upper()}")
                if health['active_alerts'] > 0:
                    print(f"  Active Alerts: {health['active_alerts']}")
            
            print("="*80)
        except UnicodeEncodeError:
            # Fallback –¥–ª—è Windows –±–µ–∑ UTF-8
            print("\n" + "="*80)
            print("PARSING REPORT (simplified)")
            print("="*80)
            print(f"Runtime: {stats.get('runtime_seconds', 0)}s")
            print(f"Found: {stats.get('total_found', 0)} | Saved: {stats.get('total_saved', 0)} | Filtered: {stats.get('total_filtered', 0)}")
            print("="*80)


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='Ultimate unified vacancy parser')
    
    parser.add_argument('--db', default='data/vacancies.db', help='Database path')
    parser.add_argument('--query', default='–¥–∏–∑–∞–π–Ω–µ—Ä', help='Search query')
    parser.add_argument('--pages', type=int, default=2, help='Pages per source')
    parser.add_argument('--sources', nargs='+', choices=['hh', 'habr'], help='Sources to parse')
    parser.add_argument('--extract-details', action='store_true', default=True, help='Extract full details')
    parser.add_argument('--use-playwright', action='store_true', help='Use Playwright for HH')
    parser.add_argument('--cache-ttl', type=int, default=1800, help='Cache TTL in seconds')
    parser.add_argument('--verbose', action='store_true', help='Verbose logging')
    parser.add_argument('--quiet', action='store_true', help='Quiet mode')
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    log_level = logging.DEBUG if args.verbose else (logging.WARNING if args.quiet else logging.INFO)
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('ultimate_parser.log', encoding='utf-8')
        ]
    )
    
    try:
        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä
        ultimate_parser = UltimateUnifiedParser(
            db_path=args.db,
            use_playwright=args.use_playwright
        )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫—ç—à
        ultimate_parser.cache_ttl = args.cache_ttl
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
        stats = await ultimate_parser.run_full_parsing(
            query=args.query,
            pages_per_source=args.pages,
            extract_details=args.extract_details,
            sources=args.sources
        )
        
        # –í—ã–≤–æ–¥–∏–º –æ—Ç—á–µ—Ç
        ultimate_parser.print_comprehensive_report(stats)
        
        return 0
        
    except Exception as e:
        logging.error(f"Critical error: {e}")
        return 1


if __name__ == "__main__":
    exit(asyncio.run(main()))
