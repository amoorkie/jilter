#!/usr/bin/env python3
"""
–ï–¥–∏–Ω—ã–π Python –ø–∞—Ä—Å–µ—Ä –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∏–∑–∞–π–Ω–µ—Ä—Å–∫–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–∞—Ä—Å–µ—Ä—ã: HH.ru, HireHi, Habr Career, GetMatch, Geekjob
–í–µ—Ä—Å–∏—è: 1.0.0
–ê–≤—Ç–æ—Ä: AI Assistant
–î–∞—Ç–∞: 2025-01-02
"""

import os
import sys
import time
import json
import sqlite3
import logging
import argparse
from datetime import datetime
from typing import List, Dict, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Python 3.13
try:
    import inspect
    if not hasattr(inspect, 'getargspec'):
        # –î–æ–±–∞–≤–ª—è–µ–º getargspec –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å pymorphy2
        def getargspec(func):
            try:
                sig = inspect.signature(func)
                args = []
                varargs = None
                varkw = None
                defaults = []
                
                for param_name, param in sig.parameters.items():
                    if param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:
                        args.append(param_name)
                        if param.default != inspect.Parameter.empty:
                            defaults.append(param.default)
                    elif param.kind == inspect.Parameter.VAR_POSITIONAL:
                        varargs = param_name
                    elif param.kind == inspect.Parameter.VAR_KEYWORD:
                        varkw = param_name
                
                return inspect.ArgSpec(args, varargs, varkw, defaults)
            except:
                return inspect.ArgSpec([], None, None, [])
        
        inspect.getargspec = getargspec
        print("‚úÖ Applied Python 3.13 compatibility fix for pymorphy2")
except Exception as e:
    print(f"‚ö†Ô∏è Compatibility fix failed: {e}")

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∏–ª—å—Ç—Ä –≤–∞–∫–∞–Ω—Å–∏–π
from vacancy_filter import filter_vacancy

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –æ—á–∏—Å—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö
try:
    from text_cleaner import clean_vacancy_data
except ImportError:
    print("WARNING: text_cleaner not found, data cleaning disabled")
    clean_vacancy_data = lambda x: x

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞
try:
    from text_normalizer import normalize_vacancy_text
except ImportError:
    print("WARNING: text_normalizer not found, text normalization disabled")
    normalize_vacancy_text = lambda x: x

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ –ø–∞—Ä—Å–µ—Ä—ã
try:
    from hh_parser import HHParser
    from hirehi_parser import HireHiParser
    from habr_parser import HabrParser
    from getmatch_parser import GetMatchParser
    from geekjob_simple import GeekjobParser
except ImportError as e:
    print(f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –ø–∞—Ä—Å–µ—Ä–æ–≤: {e}")
    print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ —Ñ–∞–π–ª—ã –ø–∞—Ä—Å–µ—Ä–æ–≤ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
    sys.exit(1)


class VacancyDatabase:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å SQLite –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π"""
    
    def __init__(self, db_path: str = "data/vacancies.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS vacancies (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        external_id TEXT UNIQUE NOT NULL,
                        source TEXT NOT NULL,
                        url TEXT NOT NULL,
                        title TEXT NOT NULL,
                        company TEXT,
                        salary TEXT,
                        location TEXT,
                        description TEXT,
                        full_description TEXT,
                        requirements TEXT,
                        tasks TEXT,
                        benefits TEXT,
                        conditions TEXT,
                        employment_type TEXT,
                        experience_level TEXT,
                        remote_type TEXT,
                        company_logo TEXT,
                        company_url TEXT,
                        published_at DATETIME,
                        created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                        updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_external_id ON vacancies(external_id)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_source ON vacancies(source)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_created_at ON vacancies(created_at)")
                
                conn.commit()
                logging.info(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {self.db_path}")
                
        except sqlite3.Error as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise
    
    def save_vacancy(self, vacancy_data: Dict[str, Any]) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        try:
            # –û—á–∏—â–∞–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
            vacancy_data = clean_vacancy_data(vacancy_data)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏
            vacancy_data = normalize_vacancy_text(vacancy_data)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –≤–∞–∫–∞–Ω—Å–∏–∏
            is_relevant, reason = filter_vacancy(vacancy_data)
            
            if not is_relevant:
                logging.info(f"üö´ –í–∞–∫–∞–Ω—Å–∏—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–∞: {vacancy_data.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')} - {reason}")
                return False
            
            logging.info(f"‚úÖ –í–∞–∫–∞–Ω—Å–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {vacancy_data.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                cursor.execute(
                    "SELECT id FROM vacancies WHERE external_id = ?",
                    (vacancy_data['external_id'],)
                )
                
                if cursor.fetchone():
                    logging.debug(f"–í–∞–∫–∞–Ω—Å–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {vacancy_data['external_id']}")
                    return False
                
                cursor.execute("""
                    INSERT INTO vacancies (
                        external_id, source, url, title, company, location,
                        description, salary_min, salary_max, salary_currency, published_at,
                        ai_specialization, ai_employment, ai_experience, ai_technologies,
                        ai_salary_min, ai_salary_max, ai_remote, ai_relevance_score, ai_summary,
                        is_approved, is_rejected, moderation_notes, moderated_by,
                        full_description, requirements, tasks, benefits, conditions,
                        company_logo, company_url, employment_type, experience_level, remote_type
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    vacancy_data['external_id'],
                    vacancy_data.get('source', ''),
                    vacancy_data['url'],
                    vacancy_data['title'],
                    vacancy_data.get('company', ''),
                    vacancy_data.get('location', ''),
                    vacancy_data.get('description', ''),
                    None,      # salary_min
                    None,      # salary_max
                    'RUB',     # salary_currency
                    vacancy_data.get('published_at'),
                    'design',  # ai_specialization
                    '[]',      # ai_employment
                    'junior',  # ai_experience
                    '[]',      # ai_technologies
                    None,      # ai_salary_min
                    None,      # ai_salary_max
                    False,     # ai_remote
                    0.8,       # ai_relevance_score
                    '–î–∏–∑–∞–π–Ω–µ—Ä—Å–∫–∞—è –≤–∞–∫–∞–Ω—Å–∏—è',  # ai_summary
                    False,     # is_approved
                    False,     # is_rejected
                    '',        # moderation_notes
                    '',        # moderated_by
                    vacancy_data.get('full_description', ''),
                    vacancy_data.get('requirements', ''),
                    vacancy_data.get('tasks', ''),
                    vacancy_data.get('benefits', ''),
                    vacancy_data.get('conditions', ''),
                    vacancy_data.get('company_logo', ''),
                    vacancy_data.get('company_url', ''),
                    vacancy_data.get('employment_type', ''),
                    vacancy_data.get('experience_level', ''),
                    vacancy_data.get('remote_type', '')
                ))
                
                conn.commit()
                logging.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—è: {vacancy_data['title']} - {vacancy_data.get('company', 'N/A')}")
                return True
                
        except sqlite3.Error as e:
            logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏: {e}")
            return False
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                cursor.execute("SELECT COUNT(*) FROM vacancies")
                total = cursor.fetchone()[0]
                
                # –ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
                cursor.execute("""
                    SELECT source, COUNT(*) FROM vacancies 
                    GROUP BY source
                """)
                by_source = dict(cursor.fetchall())
                
                # –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
                cursor.execute("""
                    SELECT COUNT(*) FROM vacancies 
                    WHERE created_at > datetime('now', '-1 day')
                """)
                last_24h = cursor.fetchone()[0]
                
                return {
                    'total': total,
                    'by_source': by_source,
                    'last_24h': last_24h
                }
                
        except sqlite3.Error as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}


class UnifiedParser:
    """–ï–¥–∏–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    def __init__(self, db_path: str = "data/vacancies.db", delay: float = 1.0):
        self.db = VacancyDatabase(db_path)
        self.delay = delay
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä—Å–µ—Ä—ã
        self.parsers = {
            'hh': HHParser(delay=delay),
            'hirehi': HireHiParser(delay=delay),
            'habr': HabrParser(delay=delay),
            'getmatch': GetMatchParser(delay=delay),
            'geekjob': GeekjobParser(delay=delay)
        }
    
    async def parse_source(self, source_name: str, parser, query: str, pages: int, extract_details: bool) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        try:
            logging.info(f"–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source_name}")
            start_time = time.time()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–µ—Ç–æ–¥ async
            if hasattr(parser.parse_vacancies, '__code__') and 'async' in str(parser.parse_vacancies.__code__.co_flags):
                vacancies = await parser.parse_vacancies(
                    query=query,
                    pages=pages,
                    extract_details=extract_details
                )
            else:
                vacancies = parser.parse_vacancies(
                    query=query,
                    pages=pages,
                    extract_details=extract_details
                )
            
            end_time = time.time()
            duration = end_time - start_time
            
            logging.info(f"{source_name}: –Ω–∞–π–¥–µ–Ω–æ {len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π –∑–∞ {duration:.2f} —Å–µ–∫")
            return vacancies
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source_name}: {e}")
            return []
    
    async def parse_all_sources(self, 
                         query: str = '–¥–∏–∑–∞–π–Ω–µ—Ä', 
                         pages_per_source: int = 3, 
                         extract_details: bool = True,
                         sources: Optional[List[str]] = None,
                         parallel: bool = True) -> Dict[str, List[Dict[str, Any]]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        
        if sources is None:
            sources = list(self.parsers.keys())
        
        logging.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        logging.info(f"–ò—Å—Ç–æ—á–Ω–∏–∫–∏: {', '.join(sources)}")
        logging.info(f"–ó–∞–ø—Ä–æ—Å: '{query}', —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫: {pages_per_source}")
        logging.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π: {extract_details}, –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ: {parallel}")
        
        results = {}
        
        if parallel:
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å asyncio
            import asyncio
            tasks = []
            
            for source_name in sources:
                if source_name in self.parsers:
                    parser = self.parsers[source_name]
                    task = self.parse_source(
                        source_name, parser, query, pages_per_source, extract_details
                    )
                    tasks.append((source_name, task))
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            for source_name, task in tasks:
                try:
                    vacancies = await task
                    results[source_name] = vacancies
                except Exception as e:
                    logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source_name}: {e}")
                    results[source_name] = []
        else:
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
            for source_name in sources:
                if source_name in self.parsers:
                    parser = self.parsers[source_name]
                    vacancies = await self.parse_source(
                        source_name, parser, query, pages_per_source, extract_details
                    )
                    results[source_name] = vacancies
                    
                    # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
                    time.sleep(self.delay)
        
        return results
    
    def save_all_vacancies(self, results: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Dict[str, int]]:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–∞–∫–∞–Ω—Å–∏–π –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
        detailed_stats = {}
        
        for source_name, vacancies in results.items():
            saved_count = 0
            filtered_count = 0
            error_count = 0
            
            for vacancy in vacancies:
                try:
                    if self.db.save_vacancy(vacancy):
                        saved_count += 1
                    else:
                        # –í–∞–∫–∞–Ω—Å–∏—è –±—ã–ª–∞ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–∞ –∏–ª–∏ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                        filtered_count += 1
                except Exception as e:
                    logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ {source_name}: {e}")
                    error_count += 1
            
            detailed_stats[source_name] = {
                'found': len(vacancies),
                'saved': saved_count,
                'filtered': filtered_count,
                'errors': error_count
            }
            
            logging.info(f"{source_name}: –Ω–∞–π–¥–µ–Ω–æ {len(vacancies)}, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count}, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered_count}")
        
        return detailed_stats
    
    def export_to_json(self, filename: str = 'all_vacancies.json'):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –≤–∞–∫–∞–Ω—Å–∏–π –≤ JSON"""
        try:
            with sqlite3.connect(self.db.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT * FROM vacancies ORDER BY created_at DESC")
                
                columns = [description[0] for description in cursor.description]
                rows = cursor.fetchall()
                
                vacancies = []
                for row in rows:
                    vacancy = dict(zip(columns, row))
                    vacancies.append(vacancy)
                
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(vacancies, f, ensure_ascii=False, indent=2, default=str)
                
                logging.info(f"–≠–∫—Å–ø–æ—Ä—Ç –≤ JSON –∑–∞–≤–µ—Ä—à—ë–Ω: {filename} ({len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π)")
                return True
                
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ JSON: {e}")
            return False


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–ï–¥–∏–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–∏–∑–∞–π–Ω–µ—Ä—Å–∫–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π')
    
    parser.add_argument('--db', default='database.db', help='–ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--query', default='–¥–∏–∑–∞–π–Ω–µ—Ä', help='–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å')
    parser.add_argument('--pages', type=int, default=3, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫')
    parser.add_argument('--delay', type=float, default=1.0, help='–ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏')
    parser.add_argument('--sources', nargs='+', 
                       choices=['hh', 'hirehi', 'habr', 'getmatch', 'geekjob'],
                       help='–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')
    parser.add_argument('--no-details', action='store_true', help='–ù–µ –∏–∑–≤–ª–µ–∫–∞—Ç—å –ø–æ–ª–Ω—ã–µ –¥–µ—Ç–∞–ª–∏')
    parser.add_argument('--extract-details', action='store_true', help='–ò–∑–≤–ª–µ–∫–∞—Ç—å –ø–æ–ª–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)')
    parser.add_argument('--no-parallel', action='store_true', help='–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥')
    parser.add_argument('--export', choices=['json'], help='–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
    parser.add_argument('--verbose', action='store_true', help='–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥')
    parser.add_argument('--quiet', action='store_true', help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥')
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    log_level = logging.DEBUG if args.verbose else (logging.WARNING if args.quiet else logging.INFO)
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('unified_parser.log', encoding='utf-8')
        ]
    )
    
    try:
        # –°–æ–∑–¥–∞—ë–º –µ–¥–∏–Ω—ã–π –ø–∞—Ä—Å–µ—Ä
        unified_parser = UnifiedParser(db_path=args.db, delay=args.delay)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
        import asyncio
        results = asyncio.run(unified_parser.parse_all_sources(
            query=args.query,
            pages_per_source=args.pages,
            extract_details=args.extract_details or not args.no_details,
            sources=args.sources,
            parallel=not args.no_parallel
        ))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        saved_counts = unified_parser.save_all_vacancies(results)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = unified_parser.db.get_statistics()
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print("\n" + "=" * 60)
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–ê–†–°–ò–ù–ì–ê –í–°–ï–• –ò–°–¢–û–ß–ù–ò–ö–û–í")
        print("=" * 60)
        
        total_found = sum(len(vacancies) for vacancies in results.values())
        total_saved = sum(stats.get('saved', 0) for stats in saved_counts.values())
        total_filtered = sum(stats.get('filtered', 0) for stats in saved_counts.values())
        
        print(f"–ù–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {total_found}")
        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î: {total_saved}")
        print(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {total_filtered}")
        print(f"–ü—Ä–æ—Ü–µ–Ω—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {(total_saved / total_found * 100) if total_found > 0 else 0:.1f}%")
        print()
        
        print("–ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
        for source_name in results.keys():
            stats = saved_counts.get(source_name, {})
            found = stats.get('found', 0)
            saved = stats.get('saved', 0)
            filtered = stats.get('filtered', 0)
            errors = stats.get('errors', 0)
            print(f"  {source_name:10}: –Ω–∞–π–¥–µ–Ω–æ {found:3}, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved:3}, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {filtered:3}, –æ—à–∏–±–æ–∫ {errors:2}")
        
        print()
        print("–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–î:")
        print(f"  –í—Å–µ–≥–æ –≤–∞–∫–∞–Ω—Å–∏–π: {stats.get('total', 0)}")
        print(f"  –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24—á: {stats.get('last_24h', 0)}")
        
        if stats.get('by_source'):
            print("  –ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –≤ –ë–î:")
            for source, count in stats['by_source'].items():
                print(f"    {source:10}: {count}")
        
        # –≠–∫—Å–ø–æ—Ä—Ç
        if args.export:
            if args.export == 'json':
                unified_parser.export_to_json()
        
        print("\n–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ!")
        return 0
        
    except KeyboardInterrupt:
        logging.info("–ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return 1
    except Exception as e:
        logging.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
